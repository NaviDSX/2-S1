{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-tuning DetrForSegmentation on custom dataset - end-to-end approach.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ec8896510738441f933654d81d197780": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_67af3fb27e52496e94bdea96916e1ea1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_33dc3bb2e2fb430e8e5861a960139b29",
              "IPY_MODEL_1aa7c8a27f6b4bd3b0cdd0033302ab20",
              "IPY_MODEL_6fc1dcd9cc5548ff9c0086e45cb74867"
            ]
          }
        },
        "67af3fb27e52496e94bdea96916e1ea1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "33dc3bb2e2fb430e8e5861a960139b29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_83de613e07d64e47af381f64cf80cec5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e891d4cce70b4071b81ec79193e71ee9"
          }
        },
        "1aa7c8a27f6b4bd3b0cdd0033302ab20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7a71969cc26649b0be9cfa528ffe4406",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 273,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 273,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d51dd7039168484e883ff1de8725ee65"
          }
        },
        "6fc1dcd9cc5548ff9c0086e45cb74867": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6d0e7d6a586b4475be457b9f8744ae16",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 273/273 [00:00&lt;00:00, 4.70kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e0c91d065a247a8bea2eeec33c6b8d8"
          }
        },
        "83de613e07d64e47af381f64cf80cec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e891d4cce70b4071b81ec79193e71ee9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7a71969cc26649b0be9cfa528ffe4406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d51dd7039168484e883ff1de8725ee65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6d0e7d6a586b4475be457b9f8744ae16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e0c91d065a247a8bea2eeec33c6b8d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a72571e0763b40d48c1326f2dd121bfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_46131af47fe248199f5a7b8777932faa",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fc640f77e783410e85a2fe2bab9405e8",
              "IPY_MODEL_e74993bb286545b791d53a71cbb49a3a",
              "IPY_MODEL_558dbd9755a94e728555b3a22fb7116a"
            ]
          }
        },
        "46131af47fe248199f5a7b8777932faa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fc640f77e783410e85a2fe2bab9405e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a83e2b273ce5402381d267e0bd2c75d4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "  5%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5c4b3b5c0fbd4d2e92469b8b79f4c52d"
          }
        },
        "e74993bb286545b791d53a71cbb49a3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9daf787b303940edaa20f5ec4ddb8249",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 53,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1e546a879d3d43949d1d555ad56ea898"
          }
        },
        "558dbd9755a94e728555b3a22fb7116a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1d4443b2a3884b6cb62c558fbe745f02",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 53/1000 [03:50&lt;1:03:09,  4.00s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_29b869d373ee42e58a635d61deb0667f"
          }
        },
        "a83e2b273ce5402381d267e0bd2c75d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5c4b3b5c0fbd4d2e92469b8b79f4c52d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9daf787b303940edaa20f5ec4ddb8249": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1e546a879d3d43949d1d555ad56ea898": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1d4443b2a3884b6cb62c558fbe745f02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "29b869d373ee42e58a635d61deb0667f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NaviDSX/2-S1/blob/main/DETR/BFinal_Fine_tuning_DetrForSegmentation_on_custom_dataset_end_to_end_approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2P6C0ezo6gC",
        "outputId": "d990296a-b3d0-466d-c922-0cea70d93a43"
      },
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git timm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 376 kB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 39.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 52.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 34.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.4 MB/s \n",
            "\u001b[?25h  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zohBiOcjGnzB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52b6a37b-f95b-4c37-a598-5cc78bba8333"
      },
      "source": [
        "!pip install -q pytorch-lightning"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 925 kB 5.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 36.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 125 kB 43.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 282 kB 45.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 34.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 160 kB 45.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 43.0 MB/s \n",
            "\u001b[?25h  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2Hnog6JX9JB",
        "outputId": "0195b717-28cc-4b8b-9b70-2222324fa753"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPk2Fm_fGWad"
      },
      "source": [
        "import torch\n",
        "torch.set_grad_enabled(False);\n",
        "import json\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class CocoPanoptic(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_folder,feature_extractor):\n",
        "        self.img_folder = img_folder\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.idfn = [x for x in os.listdir(img_folder)]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = Path(self.img_folder)/self.idfn[idx].replace('.png', '.jpg')\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        \n",
        "        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)\n",
        "        encoding = self.feature_extractor(images=img, return_tensors=\"pt\")\n",
        "        pixel_values = encoding[\"pixel_values\"].squeeze() # remove batch dimension\n",
        "        #target = encoding[\"labels\"][0] \n",
        "        return pixel_values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idfn)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hex48F1OGWMe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ec8896510738441f933654d81d197780",
            "67af3fb27e52496e94bdea96916e1ea1",
            "33dc3bb2e2fb430e8e5861a960139b29",
            "1aa7c8a27f6b4bd3b0cdd0033302ab20",
            "6fc1dcd9cc5548ff9c0086e45cb74867",
            "83de613e07d64e47af381f64cf80cec5",
            "e891d4cce70b4071b81ec79193e71ee9",
            "7a71969cc26649b0be9cfa528ffe4406",
            "d51dd7039168484e883ff1de8725ee65",
            "6d0e7d6a586b4475be457b9f8744ae16",
            "6e0c91d065a247a8bea2eeec33c6b8d8"
          ]
        },
        "outputId": "c5acb4d8-30ee-40ed-eda3-fff358d37265"
      },
      "source": [
        "import os\n",
        "from transformers import DetrFeatureExtractor\n",
        "import numpy as np\n",
        "feature_extractor = DetrFeatureExtractor.from_pretrained(\"facebook/detr-resnet-50-panoptic\", size=500, max_size=600)\n",
        "dataset2 = CocoPanoptic(img_folder='/content/drive/MyDrive/coco/val2017',feature_extractor=feature_extractor)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec8896510738441f933654d81d197780",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/273 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f5bMQwWGWJh"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn2(batch):\n",
        "  pixel_values = [item for item in batch]\n",
        "  encoded_input = feature_extractor.pad_and_create_pixel_mask(pixel_values, return_tensors=\"pt\")\n",
        "  #labels = [item[1] for item in batch]\n",
        "  batch = {}\n",
        "  batch['pixel_values'] = encoded_input['pixel_values']\n",
        "  batch['pixel_mask'] = encoded_input['pixel_mask']\n",
        "  #batch['labels'] = labels\n",
        "  return batch"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-CymRS-GV_T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "7034ff87-4090-4c85-ef41-ce27e3c8ce5a"
      },
      "source": [
        "from transformers import DetrConfig, DetrForSegmentation\n",
        "model = DetrForSegmentation.from_pretrained(\"facebook/detr-resnet-50-panoptic\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f2a26ebe7d645c3ac12eb8e5f7706b7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/11.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c2329aa2bbc41eeb05f646cfe3b425a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/164M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet50_ram-a26f946b.pth\" to /root/.cache/torch/hub/checkpoints/resnet50_ram-a26f946b.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXlHv43vRjlu"
      },
      "source": [
        "def osize(A):\n",
        "    ridx,cidx=A.shape\n",
        "    for x in reversed(range(cidx-1)):\n",
        "        if torch.all(A[:,cidx-1] == 0):\n",
        "            cidx=cidx-1\n",
        "        else:\n",
        "            break\n",
        "    for x in reversed(range(ridx-1)):\n",
        "        if torch.all(A[ridx-1,:] == 0):\n",
        "            ridx=ridx-1\n",
        "        else:\n",
        "            break\n",
        "    return (ridx,cidx)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "u2PFUSdTRjgf",
        "outputId": "d366d390-c934-4dc7-ec84-7a0419e8308e"
      },
      "source": [
        "import time\n",
        "time.ctime()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Fri Oct  8 07:03:35 2021'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMWtCBBERjdN",
        "outputId": "667aae5c-1201-4058-c904-8ca78199c560"
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1295"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0yJ8RQTuHTI"
      },
      "source": [
        "dataloader2 = DataLoader(dataset2, collate_fn=collate_fn2, batch_size=5, shuffle=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a72571e0763b40d48c1326f2dd121bfe",
            "46131af47fe248199f5a7b8777932faa",
            "fc640f77e783410e85a2fe2bab9405e8",
            "e74993bb286545b791d53a71cbb49a3a",
            "558dbd9755a94e728555b3a22fb7116a",
            "a83e2b273ce5402381d267e0bd2c75d4",
            "5c4b3b5c0fbd4d2e92469b8b79f4c52d",
            "9daf787b303940edaa20f5ec4ddb8249",
            "1e546a879d3d43949d1d555ad56ea898",
            "1d4443b2a3884b6cb62c558fbe745f02",
            "29b869d373ee42e58a635d61deb0667f"
          ]
        },
        "id": "H8bEaiWBL7WI",
        "outputId": "9dc6d9b5-4924-41aa-f07a-23ba0f245012"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.set_grad_enabled(False);\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(time.ctime())\n",
        "print(\"Running evaluation...\")\n",
        "li=[]\n",
        "for idx, batch in enumerate(tqdm(dataloader2)):\n",
        "    # get the inputs\n",
        "    print(f'idx is {idx}')\n",
        "    pixel_values = batch[\"pixel_values\"].to(device)\n",
        "    pixel_mask = batch[\"pixel_mask\"].to(device)\n",
        "    print(pixel_values.shape,pixel_mask.shape)\n",
        "    #print(batch[\"pixel_mask\"].shape)\n",
        "    ots = torch.stack([torch.as_tensor(osize(pixel_mask[i,:,:])) for i in range(len(pixel_mask))], dim=0)\n",
        "    #labels = [{k: v.to(device) for k, v in t.items()} for t in batch[\"labels\"]] # these are in DETR format, resized + normalized\n",
        "\n",
        "    # forward pass\n",
        "    with torch.no_grad():\n",
        "      outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
        "      print(outputs['logits'].shape, outputs['pred_boxes'].shape, outputs['pred_masks'].shape)\n",
        "\n",
        "    # # object detection evaluation\n",
        "    # orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
        "    # results = feature_extractor.post_process(outputs, orig_target_sizes) # convert outputs of model to COCO api\n",
        "    # target_sizes = torch.stack([t[\"size\"] for t in labels], dim=0)\n",
        "    # results = feature_extractor.post_process_segmentation(results, outputs, orig_target_sizes, target_sizes)\n",
        "    # res = {target['image_id'].item(): output for target, output in zip(labels, results)}\n",
        "    # coco_evaluator.update(res)\n",
        "\n",
        "    # panoptic segmentation evaluation\n",
        "    #target_sizes = torch.stack([t[\"size\"] for t in labels], dim=0)\n",
        "    results_panoptic = feature_extractor.post_process_panoptic(outputs,ots) # convert outputs of model to COCO api\n",
        "    navi=(pixel_values,pixel_mask,ots,outputs,results_panoptic)\n",
        "    li.append(navi)\n",
        "    print(time.ctime())\n",
        "\n",
        "    #import torch\n",
        "\n",
        "    # setting device on GPU if available, else CPU\n",
        "    #device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print('Using device:', device)\n",
        "    print()\n",
        "\n",
        "    #Additional Info when using cuda\n",
        "    if device.type == 'cuda':\n",
        "        print(torch.cuda.get_device_name(0))\n",
        "        print('Memory Usage:')\n",
        "        print('Allocated:      ', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "        print('Reserved/Cached:', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
        "#     for i, target in enumerate(labels):\n",
        "#         image_id = target[\"image_id\"].item()\n",
        "#         file_name = f\"{image_id:012d}.png\"\n",
        "#         results_panoptic[i][\"image_id\"] = image_id\n",
        "#         results_panoptic[i][\"file_name\"] = file_name\n",
        "#     panoptic_evaluator.update(results_panoptic)\n",
        "\n",
        "# coco_evaluator.synchronize_between_processes()\n",
        "# coco_evaluator.accumulate()\n",
        "# coco_evaluator.summarize()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running evaluation...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a72571e0763b40d48c1326f2dd121bfe",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "idx is 0\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    9.8 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/memory.py:375: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
            "  FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "idx is 1\n",
            "torch.Size([5, 3, 599, 600]) torch.Size([5, 599, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 2\n",
            "torch.Size([5, 3, 600, 599]) torch.Size([5, 600, 599])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 0.4 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 3\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 0.5 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 4\n",
            "torch.Size([5, 3, 450, 600]) torch.Size([5, 450, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 113, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 0.5 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 5\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 0.6 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 6\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 0.7 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 7\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 0.8 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 8\n",
            "torch.Size([5, 3, 599, 600]) torch.Size([5, 599, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 0.8 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 9\n",
            "torch.Size([5, 3, 500, 600]) torch.Size([5, 500, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 125, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 0.9 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 10\n",
            "torch.Size([5, 3, 500, 600]) torch.Size([5, 500, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 125, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 1.0 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 11\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 1.1 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 12\n",
            "torch.Size([5, 3, 450, 600]) torch.Size([5, 450, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 113, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 1.1 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 13\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 1.2 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 14\n",
            "torch.Size([5, 3, 599, 600]) torch.Size([5, 599, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 1.3 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 15\n",
            "torch.Size([5, 3, 450, 600]) torch.Size([5, 450, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 113, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 1.3 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 16\n",
            "torch.Size([5, 3, 450, 600]) torch.Size([5, 450, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 113, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 1.4 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 17\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 1.5 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 18\n",
            "torch.Size([5, 3, 599, 600]) torch.Size([5, 599, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 1.5 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 19\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 1.6 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 20\n",
            "torch.Size([5, 3, 599, 600]) torch.Size([5, 599, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 1.7 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 21\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 1.8 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 22\n",
            "torch.Size([5, 3, 450, 600]) torch.Size([5, 450, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 113, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 1.8 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 23\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 1.9 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 24\n",
            "torch.Size([5, 3, 500, 600]) torch.Size([5, 500, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 125, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 2.0 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 25\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 2.1 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 26\n",
            "torch.Size([5, 3, 500, 600]) torch.Size([5, 500, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 125, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 2.1 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 27\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 2.2 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 28\n",
            "torch.Size([5, 3, 599, 600]) torch.Size([5, 599, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 2.3 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 29\n",
            "torch.Size([5, 3, 450, 600]) torch.Size([5, 450, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 113, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 2.3 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 30\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 2.4 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 31\n",
            "torch.Size([5, 3, 500, 600]) torch.Size([5, 500, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 125, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 2.5 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 32\n",
            "torch.Size([5, 3, 450, 600]) torch.Size([5, 450, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 113, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 2.6 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 33\n",
            "torch.Size([5, 3, 599, 600]) torch.Size([5, 599, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 2.6 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 34\n",
            "torch.Size([5, 3, 599, 600]) torch.Size([5, 599, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 2.7 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 35\n",
            "torch.Size([5, 3, 488, 600]) torch.Size([5, 488, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 122, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 2.8 GB\n",
            "Cached:    8.5 GB\n",
            "idx is 36\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 2.9 GB\n",
            "Cached:    9.8 GB\n",
            "idx is 37\n",
            "torch.Size([5, 3, 599, 600]) torch.Size([5, 599, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 2.9 GB\n",
            "Cached:    9.8 GB\n",
            "idx is 38\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 3.0 GB\n",
            "Cached:    9.8 GB\n",
            "idx is 39\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 3.1 GB\n",
            "Cached:    9.8 GB\n",
            "idx is 40\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 3.2 GB\n",
            "Cached:    9.8 GB\n",
            "idx is 41\n",
            "torch.Size([5, 3, 561, 600]) torch.Size([5, 561, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 141, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 3.2 GB\n",
            "Cached:    9.8 GB\n",
            "idx is 42\n",
            "torch.Size([5, 3, 599, 600]) torch.Size([5, 599, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 3.3 GB\n",
            "Cached:    9.8 GB\n",
            "idx is 43\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 3.4 GB\n",
            "Cached:    9.8 GB\n",
            "idx is 44\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 3.5 GB\n",
            "Cached:    9.8 GB\n",
            "idx is 45\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 3.6 GB\n",
            "Cached:    9.9 GB\n",
            "idx is 46\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 3.6 GB\n",
            "Cached:    9.9 GB\n",
            "idx is 47\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 3.7 GB\n",
            "Cached:    9.9 GB\n",
            "idx is 48\n",
            "torch.Size([5, 3, 454, 600]) torch.Size([5, 454, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 114, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 3.8 GB\n",
            "Cached:    9.9 GB\n",
            "idx is 49\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 3.8 GB\n",
            "Cached:    9.9 GB\n",
            "idx is 50\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 3.9 GB\n",
            "Cached:    9.9 GB\n",
            "idx is 51\n",
            "torch.Size([5, 3, 600, 600]) torch.Size([5, 600, 600])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 4.0 GB\n",
            "Cached:    9.9 GB\n",
            "idx is 52\n",
            "torch.Size([5, 3, 599, 599]) torch.Size([5, 599, 599])\n",
            "torch.Size([5, 100, 251]) torch.Size([5, 100, 4]) torch.Size([5, 100, 150, 150])\n",
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 4.1 GB\n",
            "Cached:    9.9 GB\n",
            "idx is 53\n",
            "torch.Size([5, 3, 599, 600]) torch.Size([5, 599, 600])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-d901d18f3a8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixel_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pred_boxes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pred_masks'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/detr/modeling_detr.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, pixel_mask, decoder_attention_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mbbox_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1621\u001b[0;31m         \u001b[0mseg_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojected_feature_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m         \u001b[0mpred_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseg_masks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_queries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_masks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_masks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/detr/modeling_detr.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, bbox_mask, fpns)\u001b[0m\n\u001b[1;32m   1761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcur_fpn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m             \u001b[0mcur_fpn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_fpn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mcur_fpn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1763\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_fpn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_fpn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nearest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1764\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlay5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgn5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.34 GiB (GPU 0; 11.17 GiB total capacity; 7.49 GiB already allocated; 771.81 MiB free; 9.86 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H79qvdsQx1xL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8OSiAs4x1un"
      },
      "source": [
        "hello+5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9zCFB0dx1rd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPAgWKGPx1oh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIjovVD5x1i7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM7C5R7Vx1fZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udO-gOWCx1bl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIepDMvmHoT5"
      },
      "source": [
        "As we can see, to initialize the dataset, 3 things need to be provided besides the feature extractor:\n",
        "* the path to a directory containing the images.\n",
        "* the path to a directory containing the segmentation masks (PNG files).\n",
        "* the path to the annotation file (JSON), containing the annotations in COCO format.\n",
        "\n",
        "You can download the data from the [official website](https://cocodataset.org/#download) (under \"Dataset\" -> \"Download\" -> \"2017 Val images [5K/1GB]\" and \"2017 Panoptic Train/Val annotations [821MB]\"). I've downloaded the data, and stored it in my personal Google Drive.\n",
        "\n",
        "As the training dataset is very big, we use Numpy to randomly pick a small portion of it, for demonstration purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXr3YjLlUH1T"
      },
      "source": [
        "from transformers import DetrFeatureExtractor\n",
        "import numpy as np\n",
        "\n",
        "# we reduce the size and max_size to be able to fit the batches in GPU memory\n",
        "feature_extractor = DetrFeatureExtractor.from_pretrained(\"facebook/detr-resnet-50-panoptic\", size=500, max_size=600)\n",
        "\n",
        "dataset = CocoPanoptic(img_folder='/content/drive/MyDrive/coco/val2017', \n",
        "                             ann_folder='/content/drive/MyDrive/coco/panoptic_val2017',\n",
        "                             ann_file='/content/drive/MyDrive/coco/panoptic_val2017.json',\n",
        "                             feature_extractor=feature_extractor)\n",
        "\n",
        "# let's split it up into very tiny training and validation sets using random indices\n",
        "np.random.seed(42)\n",
        "indices = np.random.randint(low=0, high=len(dataset), size=50)\n",
        "train_dataset = torch.utils.data.Subset(dataset, indices[:40])\n",
        "val_dataset = torch.utils.data.Subset(dataset, indices[40:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrpCKYkqahno"
      },
      "source": [
        "feature_extractor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4v8mv8rahiU"
      },
      "source": [
        "vars(feature_extractor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c_1LjRDahem"
      },
      "source": [
        "dataset[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OMF-7bMkNUb"
      },
      "source": [
        "type(encoding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jlFIKo3kNRU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QyzHpa_kNLu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmjMpTkm8j6B"
      },
      "source": [
        "pixel_values, target = train_dataset[2]\n",
        "print(pixel_values.shape)\n",
        "print(target.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MuKOTp6I_GM"
      },
      "source": [
        "We pick 40 examples for training, and 10 for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDq01s0IUo3t"
      },
      "source": [
        "print(\"Number of training examples:\", len(train_dataset))\n",
        "print(\"Number of validation examples:\", len(val_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfI6pHmd2K7t"
      },
      "source": [
        "Next, let's create corresponding dataloaders. We define a custom `collate_fn`, which will batch the images (and labels) together. As images can have different sizes, images in a batch are padded up to the largest one, and a `pixel_mask` is created, indicating which pixels are real/which are padding. We can achieve this using the `pad_and_create_pixel_mask` method of `DetrFeatureExtractor`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJjrd5vp2PWe"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(batch):\n",
        "  pixel_values = [item[0] for item in batch]\n",
        "  encoded_input = feature_extractor.pad_and_create_pixel_mask(pixel_values, return_tensors=\"pt\")\n",
        "  labels = [item[1] for item in batch]\n",
        "  batch = {}\n",
        "  batch['pixel_values'] = encoded_input['pixel_values']\n",
        "  batch['pixel_mask'] = encoded_input['pixel_mask']\n",
        "  batch['labels'] = labels\n",
        "  return batch\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=3, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oO3nq0SOE5c8"
      },
      "source": [
        "type(train_dataset[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P9Y6DSKE5Zm"
      },
      "source": [
        "c=next(iter(train_dataloader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A45u7jVE5WA"
      },
      "source": [
        "type(c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdDEknGuE5Rd"
      },
      "source": [
        "len(train_dataset[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIQ5bni-E5Ov"
      },
      "source": [
        "train_dataset[0][0], type(train_dataset[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI6huiiTE5J9"
      },
      "source": [
        "train_dataset[0][1], type(train_dataset[0][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoiSXDYCE5HP"
      },
      "source": [
        "train_dataset[0][1].keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOP6PGwBE5ED"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R6r-Sr0GWPm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doJBReK3TSrv"
      },
      "source": [
        "dataloader2 = DataLoader(dataset2, collate_fn=collate_fn2, batch_size=20, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkTzatNmiss5"
      },
      "source": [
        "batch.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBL0pKQiism-"
      },
      "source": [
        "dataset2[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZVKt4ftGWE6"
      },
      "source": [
        "next(iter(dataloader2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uI7JTLZGWB7"
      },
      "source": [
        "type(dataset2[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRWbiq56Rjty"
      },
      "source": [
        "val_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEGz3Su7Rjpz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwCRc2SCtMFW"
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m23YTqQYlYJl"
      },
      "source": [
        "len(outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8747V7TflYGK"
      },
      "source": [
        "outputs[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGeUJEnIlYCm"
      },
      "source": [
        "len(results_panoptic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqPijGc_lX_-"
      },
      "source": [
        "results_panoptic[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQeE7VdPlX9E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnjIAV7AlX5l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdZkmjWpuwlj"
      },
      "source": [
        "type"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl3gJfKmuwij"
      },
      "source": [
        "j=pixel_mask.to('cpu').numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHfrhR91egMa"
      },
      "source": [
        "j=pixel_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoW2arewuwdT"
      },
      "source": [
        "j.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nNq010zuwXn"
      },
      "source": [
        "k=j[0,:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruUV8iW5dj2v"
      },
      "source": [
        "k.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1LBWiIcdjzz"
      },
      "source": [
        "l=j[1,:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ6h-VHOdju6"
      },
      "source": [
        "m=torch.dstack((k,l))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNunigX-fN_J"
      },
      "source": [
        "osize(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6havUCQdjrs"
      },
      "source": [
        "m.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tApkwQ19djlv"
      },
      "source": [
        "orig_target_sizes = torch.stack([j[i,:,:] for i in range(2)], dim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G1LfH8AfDdn"
      },
      "source": [
        "orig_target_sizes.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyLb8YqwfDUw"
      },
      "source": [
        "orig_target_sizes = torch.stack([j[i,:,:] for i in range(2)], dim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn7ImgG9fDSY"
      },
      "source": [
        "ots = torch.stack([torch.as_tensor(osize(j[i,:,:])) for i in range(2)], dim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwJgO3T-h0ol"
      },
      "source": [
        "ots.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bfk6BGYh0i6"
      },
      "source": [
        "\n",
        "\n",
        "for x in pixel_mask:\n",
        "  print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvaZs_6ifDQe"
      },
      "source": [
        "torch.as_tensor(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMMhLh1HfDMq"
      },
      "source": [
        "f=(1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4SjKyBtizTz"
      },
      "source": [
        "g=torch.as_tensor(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHbtNxFmizGz"
      },
      "source": [
        "g.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shjSu9A3izDK"
      },
      "source": [
        "d=[[1],[2]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qaoo3QjWjpZR"
      },
      "source": [
        "g=torch.as_tensor(d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIckcXPLjpUq"
      },
      "source": [
        "g.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vatzEv3LjpQ0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8UouAE6fDJp"
      },
      "source": [
        "len(pixel_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n7-pYzMfDGc"
      },
      "source": [
        "ots = torch.stack([torch.as_tensor(osize(j[i,:,:])) for i in range(len(pixel_mask))], dim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmaSdUEFfDCl"
      },
      "source": [
        "ots.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qduuXaMyfC_R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD1Km_U9GV8K"
      },
      "source": [
        "pixel_values.shape, pixel_mask.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux39491rGV4V"
      },
      "source": [
        "batch[\"pixel_values\"].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuuPUKB7QKME"
      },
      "source": [
        "type(outputs), len(outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMv4YsO7QKFM"
      },
      "source": [
        "outputs.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34HkfKMLQKAv"
      },
      "source": [
        "outputs['logits'].shape, outputs['pred_boxes'].shape, outputs['pred_masks'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5ZiUvp_Q1TJ"
      },
      "source": [
        "Shape of pixel_values: torch.Size([3, 599, 402])\n",
        "Shape of logits: torch.Size([3, 100, 251])\n",
        "Shape of predicted bounding boxes: torch.Size([3, 100, 4])\n",
        "Shape of predicted masks: torch.Size([3, 100, 125, 150])\n",
        "This looks ok: the logits are of shape (batch_size, num_queries, num_classes + 1) - + 1 because there's also a \"no object\" class. The predicted bounding boxes are of shape (batch_size, num_queries, 4), since each bounding box consists of 4 coordinates (DETR outputs them in center_x, center_y, width, height format). The predicted masks logits are of shape (batch_size, num_queries, height/4, width/4)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8S_NnMTQJzT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWfo72KxE5Aa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DKgQU2pbJrJ"
      },
      "source": [
        "## Train the model using PyTorch Lightning\n",
        "\n",
        "Here we define a LightningModule, which is an `nn.Module` with some extra functionality. For a good introduction to PyTorch Lightning, I refer to its [documentation](https://pytorch-lightning.readthedocs.io/en/latest/?_ga=2.260781005.179149119.1623936252-1738348008.1615553774) as well as the official [tutorials](https://pytorch-lightning.readthedocs.io/en/latest/#tutorials).\n",
        "\n",
        "Basically, PyTorch Lightning takes care of all the boilerplate such as defining the training loop, placing tensors on the right devices, fetching data from the dataloaders, and so on. We just need to define the forward pass, the training and validation step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r-lMAWKWoLY"
      },
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "\n",
        "class DetrPanoptic(pl.LightningModule):\n",
        "\n",
        "     def __init__(self, model, lr, lr_backbone, weight_decay):\n",
        "         super().__init__()\n",
        "      \n",
        "         self.model = model\n",
        "\n",
        "         # see https://github.com/PyTorchLightning/pytorch-lightning/pull/1896\n",
        "         self.lr = lr\n",
        "         self.lr_backbone = lr_backbone\n",
        "         self.weight_decay = weight_decay\n",
        "\n",
        "     def forward(self, pixel_values, pixel_mask):\n",
        "       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
        "\n",
        "       return outputs\n",
        "     \n",
        "     def common_step(self, batch, batch_idx):\n",
        "       pixel_values = batch[\"pixel_values\"]\n",
        "       pixel_mask = batch[\"pixel_mask\"]\n",
        "       labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
        "\n",
        "       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
        "\n",
        "       loss = outputs.loss\n",
        "       loss_dict = outputs.loss_dict\n",
        "\n",
        "       return loss, loss_dict\n",
        "\n",
        "     def training_step(self, batch, batch_idx):\n",
        "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
        "        # logs metrics for each training_step,\n",
        "        # and the average across the epoch\n",
        "        self.log(\"training_loss\", loss)\n",
        "        for k,v in loss_dict.items():\n",
        "          self.log(\"train_\" + k, v.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "     def validation_step(self, batch, batch_idx):\n",
        "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
        "        self.log(\"validation_loss\", loss)\n",
        "        for k,v in loss_dict.items():\n",
        "          self.log(\"validation_\" + k, v.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "     def configure_optimizers(self):\n",
        "        param_dicts = [\n",
        "              {\"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
        "              {\n",
        "                  \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
        "                  \"lr\": self.lr_backbone,\n",
        "              },\n",
        "        ]\n",
        "        optimizer = torch.optim.AdamW(param_dicts, lr=self.lr,\n",
        "                                  weight_decay=self.weight_decay)\n",
        "        \n",
        "        return optimizer\n",
        "\n",
        "     def train_dataloader(self):\n",
        "        return train_dataloader\n",
        "\n",
        "     def val_dataloader(self):\n",
        "        return val_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJuNXnFGJdL_"
      },
      "source": [
        "Let's start up Tensorboard, which will show us some nice plots over the course of training (PyTorch Lightning uses Tensorboard as default logger)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_P9lDDwcIWh"
      },
      "source": [
        "# Start tensorboard.\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QSnUx3GTOWq"
      },
      "source": [
        "Here we load the model trained on COCO panoptic. We decide to only train the class labels classifier from scratch, and further fine-tune the bounding box regressor and mask head."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHQFVdawdgq-"
      },
      "source": [
        "from transformers import DetrConfig, DetrForSegmentation\n",
        "\n",
        "model = DetrForSegmentation.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\n",
        "state_dict = model.state_dict()\n",
        "# Remove class weights\n",
        "del state_dict[\"detr.class_labels_classifier.weight\"]\n",
        "del state_dict[\"detr.class_labels_classifier.bias\"]\n",
        "# define new model with custom class classifier\n",
        "config = DetrConfig.from_pretrained(\"facebook/detr-resnet-50-panoptic\", num_labels=250)\n",
        "model.load_state_dict(state_dict, strict=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQcaYw9rX2Ux"
      },
      "source": [
        "Next, we define the PyTorch LightningModule, and verify its outputs on a batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SGVieF4dHEK"
      },
      "source": [
        "model = DetrPanoptic(model=model, lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\n",
        "\n",
        "# pick the first training batch\n",
        "batch = next(iter(train_dataloader))\n",
        "# forward through the model\n",
        "outputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Alf4tZ34O7wt"
      },
      "source": [
        "print(\"Shape of pixel_values:\", pixel_values.shape)\n",
        "print(\"Shape of logits:\", outputs.logits.shape)\n",
        "print(\"Shape of predicted bounding boxes:\", outputs.pred_boxes.shape)\n",
        "print(\"Shape of predicted masks:\", outputs.pred_masks.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oefHUbXjYhZM"
      },
      "source": [
        "This looks ok: the logits are of shape `(batch_size, num_queries, num_classes + 1)` - + 1 because there's also a \"no object\" class. The predicted bounding boxes are of shape `(batch_size, num_queries, 4)`, since each bounding box consists of 4 coordinates (DETR outputs them in center_x, center_y, width, height format). The predicted masks logits are of shape `(batch_size, num_queries, height/4, width/4)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQIGURwuz4Xc"
      },
      "source": [
        "Next, let's train! We train for a maximum of 25 epochs, and also use gradient clipping. You can refresh Tensorboard above to see how the losses evolve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAhihLFcb34k"
      },
      "source": [
        "from pytorch_lightning import Trainer\n",
        "\n",
        "trainer = Trainer(gpus=1, max_epochs=25, gradient_clip_val=0.1)\n",
        "trainer.fit(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohWDWfb-dzeg"
      },
      "source": [
        "Note that after training, PyTorch Lightning automatically saves the checkpoint in the logging directory. You can load it back in as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmIQAc7qd6NH"
      },
      "source": [
        "# model = DetrPanoptic.load_from_checkpoint(checkpoint_path=\"/content/lightning_logs/version_2/checkpoints/epoch=1-step=57.ckpt\", \n",
        "#    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aRuAxMTcS84"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "Finally, we evaluate the model on the validation set. The original DETR repo has some nice evaluation tools that we will use (`CocoEvaluator` and `PanopticEvaluator`), which we are going to import."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnfj0SUyro-Z"
      },
      "source": [
        "! rm -r detr\n",
        "! git clone https://github.com/facebookresearch/detr.git\n",
        "%cd detr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTEOQo1-FJES"
      },
      "source": [
        "! pip install -q git+https://github.com/cocodataset/panopticapi.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCttXXst2ZUy"
      },
      "source": [
        "We can use `CocoEvaluator` to evaluate the predicted bounding boxes + predicted instance segmentation masks. For this, we need load the ground truth COCO detection dataset. We only need to retrieve the original COCO annotations for the images included in our validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F91bRRg4Bt0g"
      },
      "source": [
        "import torchvision\n",
        "import os\n",
        "\n",
        "class CocoDetection(torchvision.datasets.CocoDetection):\n",
        "    def __init__(self, img_folder, ann_file, feature_extractor, train=True):\n",
        "        super(CocoDetection, self).__init__(img_folder, ann_file)\n",
        "        self.feature_extractor = feature_extractor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # read in PIL image and target in COCO format\n",
        "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
        "        \n",
        "        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)\n",
        "        image_id = self.ids[idx]\n",
        "        target = {'image_id': image_id, 'annotations': target}\n",
        "        encoding = self.feature_extractor(images=img, annotations=target, return_tensors=\"pt\")\n",
        "        pixel_values = encoding[\"pixel_values\"].squeeze() # remove batch dimension\n",
        "        target = encoding[\"labels\"][0] # remove batch dimension\n",
        "\n",
        "        return pixel_values, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56D4UczGBjiu"
      },
      "source": [
        "# load ground truths\n",
        "base_ds = CocoDetection(img_folder='/content/drive/MyDrive/DETR/COCO data/val2017', \n",
        "                        ann_file='/content/drive/MyDrive/DETR/COCO data/annotations/instances_val2017.json',\n",
        "                        feature_extractor=feature_extractor, train=False).coco"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wkz681VFmMYd"
      },
      "source": [
        "from datasets.coco_eval import CocoEvaluator\n",
        "\n",
        "iou_types = ['bbox', 'segm']\n",
        "coco_evaluator = CocoEvaluator(base_ds, iou_types) # initialize evaluator with ground truths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzgmcUH8ilKm"
      },
      "source": [
        "We can use `PanopticEvaluator` to evaluate the panoptic segmentation masks.\n",
        "\n",
        "! When initializing `PanopticEvaluator`, we should only add the relevant panoptic segmentations (i.e. the annotations for which we are going to evaluate).\n",
        "Let's only keep the relevant panoptic segmentation annotations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tTH4a4AjhKt"
      },
      "source": [
        "import json\n",
        "\n",
        "# read in all annotations\n",
        "with open('/content/drive/MyDrive/DETR/COCO data/annotations/coco_panoptic/annotations/panoptic_val2017.json') as f:\n",
        "  data = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OMywLZIjkpJ"
      },
      "source": [
        "data.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2_QvAZoiAhG"
      },
      "source": [
        "# get image ids of images in validation set\n",
        "image_ids = []\n",
        "for batch in val_dataloader:\n",
        "  labels = batch['labels']\n",
        "  for label in labels:\n",
        "    image_ids.append(label['image_id'].item())\n",
        "print(image_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIbXMjHdhJM1"
      },
      "source": [
        "# only keep those annotations\n",
        "relevant_annotations = []\n",
        "for ann in data['annotations']:\n",
        "  if ann['image_id'] in image_ids:\n",
        "    relevant_annotations.append(ann)\n",
        "print(len(relevant_annotations))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDVGIJY7j3Pc"
      },
      "source": [
        "data['annotations'] = relevant_annotations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6b7Uwp0iqyi"
      },
      "source": [
        "# write to json file\n",
        "import json\n",
        "with open('panoptic_val.json', 'w') as f:\n",
        "    json.dump(data, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vk3l_iDlj4n"
      },
      "source": [
        "# check whether this is ok\n",
        "with open('panoptic_val.json', 'r') as f:\n",
        "        gt_json = json.load(f)\n",
        "len(gt_json['annotations'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2VWv4mLmVyN"
      },
      "source": [
        "from datasets.panoptic_eval import PanopticEvaluator\n",
        "\n",
        "# inititialiaze panoptic evaluator with the ground truth annotations\n",
        "panoptic_evaluator = PanopticEvaluator(\n",
        "            '/content/detr/panoptic_val.json',\n",
        "            val_dataloader.dataset.dataset.ann_folder,\n",
        "            output_dir=\".\",\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhxJmdrnmaU3"
      },
      "source": [
        "Finally, let's evaluate!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-ma9DPmL9t_"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"Running evaluation...\")\n",
        "\n",
        "for idx, batch in enumerate(tqdm(val_dataloader)):\n",
        "    # get the inputs\n",
        "    pixel_values = batch[\"pixel_values\"].to(device)\n",
        "    pixel_mask = batch[\"pixel_mask\"].to(device)\n",
        "    labels = [{k: v.to(device) for k, v in t.items()} for t in batch[\"labels\"]] # these are in DETR format, resized + normalized\n",
        "\n",
        "    # forward pass\n",
        "    with torch.no_grad():\n",
        "      outputs = model.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
        "\n",
        "    # object detection evaluation\n",
        "    orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
        "    results = feature_extractor.post_process(outputs, orig_target_sizes) # convert outputs of model to COCO api\n",
        "    target_sizes = torch.stack([t[\"size\"] for t in labels], dim=0)\n",
        "    results = feature_extractor.post_process_segmentation(results, outputs, orig_target_sizes, target_sizes)\n",
        "    res = {target['image_id'].item(): output for target, output in zip(labels, results)}\n",
        "    coco_evaluator.update(res)\n",
        "\n",
        "    # panoptic segmentation evaluation\n",
        "    target_sizes = torch.stack([t[\"size\"] for t in labels], dim=0)\n",
        "    results_panoptic = feature_extractor.post_process_panoptic(outputs, target_sizes, orig_target_sizes) # convert outputs of model to COCO api\n",
        "    for i, target in enumerate(labels):\n",
        "        image_id = target[\"image_id\"].item()\n",
        "        file_name = f\"{image_id:012d}.png\"\n",
        "        results_panoptic[i][\"image_id\"] = image_id\n",
        "        results_panoptic[i][\"file_name\"] = file_name\n",
        "    panoptic_evaluator.update(results_panoptic)\n",
        "\n",
        "coco_evaluator.synchronize_between_processes()\n",
        "coco_evaluator.accumulate()\n",
        "coco_evaluator.summarize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA-IfWwvDB6r"
      },
      "source": [
        "panoptic_evaluator.synchronize_between_processes()\n",
        "panoptic_evaluator.summarize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y72_L3llYyyU"
      },
      "source": [
        "## Inference (+ visualization)\n",
        "\n",
        "Let's visualize the predictions of DETR on one of the first images of the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5lmv_5bTO-a"
      },
      "source": [
        "#We can use the image_id in target to know which image it is\n",
        "pixel_values, target = val_dataset[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz4F-40YT00E"
      },
      "source": [
        "pixel_values = pixel_values.unsqueeze(0).to(device)\n",
        "print(pixel_values.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y1Na3QXT1p8"
      },
      "source": [
        "with torch.no_grad():\n",
        "  # forward pass to get class logits and bounding boxes\n",
        "  outputs = model(pixel_values=pixel_values, pixel_mask=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmruI5504Os3"
      },
      "source": [
        "import torch\n",
        "\n",
        "# use the post_process_panoptic method of DetrFeatureExtractor, which expects as input the target size of the predictions (which we set here to the image size)\n",
        "processed_sizes = torch.as_tensor(pixel_values.shape[-2:]).unsqueeze(0)\n",
        "result = feature_extractor.post_process_panoptic(outputs, processed_sizes)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guxAQRaWZWrq"
      },
      "source": [
        "import itertools\n",
        "import io\n",
        "import seaborn as sns\n",
        "import numpy \n",
        "from transformers.models.detr.feature_extraction_detr import rgb_to_id\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "palette = itertools.cycle(sns.color_palette())\n",
        "\n",
        "# The segmentation is stored in a special-format png\n",
        "panoptic_seg = Image.open(io.BytesIO(result['png_string']))\n",
        "panoptic_seg = numpy.array(panoptic_seg, dtype=numpy.uint8).copy()\n",
        "# We retrieve the ids corresponding to each mask\n",
        "panoptic_seg_id = rgb_to_id(panoptic_seg)\n",
        "\n",
        "# Finally we color each mask individually\n",
        "panoptic_seg[:, :, :] = 0\n",
        "for id in range(panoptic_seg_id.max() + 1):\n",
        "  panoptic_seg[panoptic_seg_id == id] = numpy.asarray(next(palette)) * 255\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.imshow(panoptic_seg)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRWtWLdnURzn"
      },
      "source": [
        "Compare to the original image and ground truth segmentation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pliSe5mGUNdE"
      },
      "source": [
        "image_id = target['image_id'].item()\n",
        "image = base_ds.loadImgs(image_id)[0]\n",
        "img = Image.open(os.path.join('/content/drive/MyDrive/DETR/COCO data/val2017', image['file_name']))\n",
        "img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG9mckzCUfmw"
      },
      "source": [
        "# replace jpg by png\n",
        "file_name = image['file_name'].replace('.jpg', '.png')\n",
        "image = Image.open(os.path.join('/content/drive/MyDrive/DETR/COCO data/annotations/coco_panoptic/panoptic_val2017', file_name))\n",
        "image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svFgt1jFUtbt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}